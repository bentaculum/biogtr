{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5593318-fecf-4a12-994b-373ea4e6bab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01/01\n",
      "TRAIN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_loss: 1.80920 | LR: 0.00010:   0%|     | 0/7 [00:01<?, ?it/s, loss=1.81]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.8092, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_loss: 1.80920 | LR: 0.00010:   0%|     | 0/7 [00:01<?, ?it/s, loss=1.81]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import sleap_io\n",
    "import torch\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from biogtr.datasets.sleap_dataset import SleapDataset\n",
    "from biogtr.datasets.microscopy_dataset import MicroscopyDataset\n",
    "from biogtr.models.global_tracking_transformer import GlobalTrackingTransformer\n",
    "from biogtr.training.losses import AssoLoss\n",
    "\n",
    "import torchvision.transforms.functional as tvf\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train_slp_files = [\"/mnt/talmodata/datasets/mot/animal/sleap/benchmarks/flies13/190719_090330_wt_18159206_rig1.2@15000-17560.slp\"]\n",
    "    train_vid_files = [\"/mnt/talmodata/datasets/mot/animal/sleap/benchmarks/flies13/190719_090330_wt_18159206_rig1.2@15000-17560.mp4\"]\n",
    "    \n",
    "    train_micro_labels = [\"/mnt/talmodata/datasets/mot/microscopy/ICY/5_cell/5_cells_1_gt.xml\"]\n",
    "    train_micro_tifs = [\"/mnt/talmodata/datasets/mot/microscopy/ICY/5_cell/5_cells_1.tif\"]\n",
    "    labels = sleap_io.load_slp(train_slp_files[0])\n",
    "\n",
    "    device = 'cpu'\n",
    "\n",
    "    feats = 256\n",
    "\n",
    "    tracking_transformer = GlobalTrackingTransformer(\n",
    "        d_model=feats,\n",
    "        num_encoder_layers=1,\n",
    "        num_decoder_layers=1,\n",
    "        dim_feedforward=feats,\n",
    "        feature_dim_attn_head=feats,\n",
    "    ).to(device)\n",
    "\n",
    "    asso_loss = AssoLoss().to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        tracking_transformer.parameters(), lr=1e-4, betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "\n",
    "    train_ds = SleapDataset(\n",
    "        train_slp_files,\n",
    "        train_vid_files,\n",
    "        padding=5,\n",
    "        crop_size=128,\n",
    "        chunk=True,\n",
    "        clip_length=32,\n",
    "        crop_type=\"centroid\",\n",
    "    )\n",
    "    train_ds = MicroscopyDataset(train_micro_tifs,\n",
    "                                 train_micro_labels,\n",
    "                                 \"ICY\",\n",
    "                                 padding=5,\n",
    "                                 crop_size=20,\n",
    "                                 chunk=True,\n",
    "                                 clip_length=32\n",
    "                                )\n",
    "    \n",
    "    instances = next(iter(train_ds))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_ds.no_batching_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    num_epochs = 1\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"Epoch: {:02d}/{:02d}\".format(epoch, num_epochs))\n",
    "\n",
    "        losses = []\n",
    "        _ = tracking_transformer.train()\n",
    "\n",
    "        print(\"TRAIN\")\n",
    "        loop = tqdm(\n",
    "            enumerate(train_loader), position=0, leave=True, total=len(train_loader)\n",
    "        )\n",
    "        for i, instances in loop:\n",
    "            instances = instances[0]  # For batch size of 1.\n",
    "\n",
    "            asso_preds = tracking_transformer(instances)\n",
    "\n",
    "            # Compute loss.\n",
    "            loss = asso_loss(asso_preds, instances)\n",
    "            print('loss: ', loss)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loop.set_description(\n",
    "                \"current_loss: {:.5f} | LR: {:.5f}\".format(\n",
    "                    loss.item(), optimizer.param_groups[0][\"lr\"]\n",
    "                )\n",
    "            )\n",
    "            loop.set_postfix(loss=np.mean(losses))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            break\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28320d6d-7b06-41f6-8761-2de080fb7260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_id': torch.int64,\n",
       " 'img_shape': torch.int64,\n",
       " 'frame_id': torch.int64,\n",
       " 'num_detected': torch.int64,\n",
       " 'gt_track_ids': torch.int64,\n",
       " 'bboxes': torch.float32,\n",
       " 'crops': torch.float32,\n",
       " 'features': torch.float32,\n",
       " 'pred_track_ids': torch.int64,\n",
       " 'asso_output': torch.float32,\n",
       " 'matches': torch.float32,\n",
       " 'traj_score': torch.float32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{key: tensor.dtype for key, tensor in instances[0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9362fa94-8b2c-4f15-8cf8-71632358311e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_id': tensor([0]),\n",
       " 'img_shape': tensor([[  1, 512, 512]]),\n",
       " 'frame_id': tensor([192]),\n",
       " 'num_detected': tensor([5]),\n",
       " 'gt_track_ids': tensor([0, 1, 2, 3, 4]),\n",
       " 'bboxes': tensor([[0.8320, 0.4023, 0.8906, 0.4609],\n",
       "         [0.8340, 0.4043, 0.8926, 0.4629],\n",
       "         [0.8164, 0.4121, 0.8750, 0.4707],\n",
       "         [0.2422, 0.5215, 0.3008, 0.5801],\n",
       "         [0.4570, 0.7148, 0.5156, 0.7734]]),\n",
       " 'crops': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       " \n",
       " \n",
       "         [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " 'features': tensor([[ 2.2376e-02,  4.4550e-02, -2.1048e-02,  ...,  6.1376e-02,\n",
       "          -5.5941e-02,  4.5259e-02],\n",
       "         [ 6.8275e-02,  6.3066e-02,  3.0601e-02,  ..., -9.1070e-02,\n",
       "           9.2710e-02,  2.9355e-02],\n",
       "         [ 3.0315e-02,  9.9285e-03, -4.0635e-02,  ..., -1.4142e-02,\n",
       "           3.4526e-02, -2.1723e-05],\n",
       "         [-4.4547e-02, -1.6910e-04,  7.0209e-02,  ..., -1.5085e-02,\n",
       "           7.2654e-03, -9.1866e-03],\n",
       "         [ 6.4099e-03,  6.4524e-03, -4.0680e-03,  ..., -1.6600e-02,\n",
       "          -5.9370e-02, -7.4963e-02]], grad_fn=<DivBackward0>),\n",
       " 'pred_track_ids': tensor([-1, -1, -1, -1, -1]),\n",
       " 'asso_output': tensor([]),\n",
       " 'matches': tensor([]),\n",
       " 'traj_score': tensor([])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc2234d-95a9-40a2-9e66-18aaf0e17b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import sleap_io\n",
    "# import torch\n",
    "# import imageio\n",
    "# import matplotlib.pyplot as plt\n",
    "# # from dataset import AnimalDataset\n",
    "# # from pynvml import *\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm\n",
    "# from biogtr.models.global_tracking_transformer import GlobalTrackingTransformer\n",
    "# from biogtr.training.losses import AssoLoss\n",
    "\n",
    "# import torchvision.transforms.functional as tvf\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# def pad_bbox(bbox, padding=16):\n",
    "#     \"\"\"Pad bounding box coordinates.\n",
    "\n",
    "#     Args:\n",
    "#         bbox: Bounding box in [y1, x1, y2, x2] format.\n",
    "#         padding: Padding to add to each side in pixels.\n",
    "\n",
    "#     Returns:\n",
    "#         Padded bounding box in [y1, x1, y2, x2] format.\n",
    "#     \"\"\"\n",
    "#     y1, x1, y2, x2 = bbox\n",
    "#     y1, x1 = y1 - padding, x1 - padding\n",
    "#     y2, x2 = y2 + padding, x2 + padding\n",
    "#     return [y1, x1, y2, x2]\n",
    "\n",
    "\n",
    "# def crop_bbox(img, bbox):\n",
    "#     \"\"\"Crop an image to a bounding box.\n",
    "\n",
    "#     Args:\n",
    "#         img: Image as a tensor of shape (channels, height, width).\n",
    "#         bbox: Bounding box in [y1, x1, y2, x2] format.\n",
    "\n",
    "#     Returns:\n",
    "#         Cropped pixels as tensor of shape (channels, height, width).\n",
    "#     \"\"\"\n",
    "#     # Crop to the bounding box.\n",
    "#     y1, x1, y2, x2 = bbox\n",
    "#     crop = tvf.crop(\n",
    "#         img,\n",
    "#         top=int(round(y1)),\n",
    "#         left=int(round(x1)),\n",
    "#         height=int(round(y2 - y1)),\n",
    "#         width=int(round(x2 - x1)),\n",
    "#     )\n",
    "\n",
    "#     return crop\n",
    "\n",
    "\n",
    "# def centroid_crop(instance, anchors, crop_size):\n",
    "#     \"\"\"Crop bbox around instance centroid. This is useful for ensuring that\n",
    "#     crops are centered around each instance in the case of incorrect pose\n",
    "#     estimates\n",
    "\n",
    "#     Args:\n",
    "#         instance: a labeled instance in a frame\n",
    "#         anchor: index of a given anchor point to use as the centroid\n",
    "#         crop_size: Integer specifying the crop height and width\n",
    "\n",
    "#     Returns:\n",
    "#         Bounding box in [y1, x1, y2, x2] format.\n",
    "#     \"\"\"\n",
    "\n",
    "#     for anchor in anchors:\n",
    "#         cx, cy = instance[anchor].x, instance[anchor].y\n",
    "#         if not np.isnan(cx):\n",
    "#             break\n",
    "\n",
    "#     bbox = [\n",
    "#         -crop_size / 2 + cy,\n",
    "#         -crop_size / 2 + cx,\n",
    "#         crop_size / 2 + cy,\n",
    "#         crop_size / 2 + cx,\n",
    "#     ]\n",
    "\n",
    "#     return bbox\n",
    "\n",
    "\n",
    "# def resize_and_pad(img, output_size):\n",
    "#     \"\"\"Resize and pad an image to fit a square output size.\n",
    "\n",
    "#     Args:\n",
    "#         img: Image as a tensor of shape (channels, height, width).\n",
    "#         output_size: Integer size of height and width of output.\n",
    "\n",
    "#     Returns:\n",
    "#         The image zero padded to be of shape (channels, output_size, output_size).\n",
    "#     \"\"\"\n",
    "#     # Figure out how to scale without breaking aspect ratio.\n",
    "#     img_height, img_width = img.shape[-2:]\n",
    "#     if img_width < img_height:  # taller\n",
    "#         crop_height = output_size\n",
    "#         scale = crop_height / img_height\n",
    "#         crop_width = int(img_width * scale)\n",
    "#     else:  # wider\n",
    "#         crop_width = output_size\n",
    "#         scale = crop_width / img_width\n",
    "#         crop_height = int(img_height * scale)\n",
    "\n",
    "#     # Scale without breaking aspect ratio.\n",
    "#     img = tvf.resize(img, size=[crop_height, crop_width])\n",
    "\n",
    "#     # Pad to square.\n",
    "#     img_height, img_width = img.shape[-2:]\n",
    "#     hp1 = int((output_size - img_width) / 2)\n",
    "#     vp1 = int((output_size - img_height) / 2)\n",
    "#     hp2 = output_size - (img_width + hp1)\n",
    "#     vp2 = output_size - (img_height + vp1)\n",
    "#     padding = (hp1, vp1, hp2, vp2)\n",
    "#     return tvf.pad(img, padding, 0, \"constant\")\n",
    "\n",
    "# class AnimalDataset(Dataset):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         slp_files,\n",
    "#         padding=5,\n",
    "#         crop_size=128,\n",
    "#         anchor_names=[\"thorax\", \"head\"],\n",
    "#         chunk=True,\n",
    "#         clip_length=500,\n",
    "#         crop_type=\"centroid\",\n",
    "#     ):\n",
    "#         self.slp_files = slp_files\n",
    "#         self.padding = padding\n",
    "#         self.crop_size = crop_size\n",
    "#         self.anchor_names = anchor_names\n",
    "#         self.chunk = chunk\n",
    "#         self.clip_length = clip_length\n",
    "#         self.crop_type = crop_type\n",
    "\n",
    "#         assert self.crop_type in [\"centroid\", \"pose\"], \"Invalid crop type!\"\n",
    "\n",
    "#         self.labels = [sleap_io.load_slp(slp_file) for slp_file in self.slp_files]\n",
    "\n",
    "#         # for label in self.labels:\n",
    "#             # label.remove_empty_instances(keep_empty_frames=False)\n",
    "\n",
    "#         self.frame_idx = [np.arange(len(label)) for label in self.labels]\n",
    "\n",
    "#         if self.chunk:\n",
    "#             self.chunks = [\n",
    "#                 [i * self.clip_length for i in range(len(label) // self.clip_length)]\n",
    "#                 for label in self.labels\n",
    "#             ]\n",
    "\n",
    "#             self.chunked_frame_idx, self.label_idx = [], []\n",
    "#             for i, (split, frame_idx) in enumerate(zip(self.chunks, self.frame_idx)):\n",
    "#                 frame_idx_split = np.split(frame_idx, split)[1:]\n",
    "#                 self.chunked_frame_idx.extend(frame_idx_split)\n",
    "#                 self.label_idx.extend(len(frame_idx_split) * [i])\n",
    "#         else:\n",
    "#             self.chunked_frame_idx = self.frame_idx\n",
    "#             self.label_idx = [i for i in range(len(self.labels))]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.chunked_frame_idx)\n",
    "\n",
    "#     def no_batching_fn(self, batch):\n",
    "#         return batch\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         label_idx = self.label_idx[idx]\n",
    "#         frame_idx = self.chunked_frame_idx[idx]\n",
    "\n",
    "#         video = self.labels[label_idx]\n",
    "\n",
    "\n",
    "#         anchors = [\n",
    "#             video.skeletons[0].node_names.index(anchor_name)\n",
    "#             for anchor_name in self.anchor_names\n",
    "#         ]\n",
    "\n",
    "\n",
    "#         video_name = os.path.splitext(self.slp_files[label_idx])[0] + \".mp4\"\n",
    "\n",
    "#         vid_reader = imageio.get_reader(video_name, 'ffmpeg')\n",
    "\n",
    "#         instances = []\n",
    "#         for i in frame_idx:\n",
    "#             gt_track_ids, poses, bboxes, crops = [], [], [], []\n",
    "\n",
    "#             i = int(i)\n",
    "\n",
    "#             lf = video[i]\n",
    "#             lf_img = vid_reader.get_data(i)\n",
    "\n",
    "#             img = tvf.to_tensor(lf_img)\n",
    "\n",
    "#             _, h, w = img.shape\n",
    "\n",
    "#             for instance in lf:\n",
    "#                 # gt_track_ids\n",
    "#                 gt_track_ids.append(video.tracks.index(instance.track))\n",
    "\n",
    "#                 # poses\n",
    "#                 poses.append(np.array(instance.numpy()).astype(\"float32\"))\n",
    "\n",
    "#                 # bboxes\n",
    "#                 if self.crop_type == \"centroid\":\n",
    "#                     bbox = pad_bbox(\n",
    "#                         centroid_crop(instance, anchors, self.crop_size),\n",
    "#                         padding=self.padding,\n",
    "#                     )\n",
    "#                 elif self.crop_type == \"pose\":\n",
    "#                     points = np.array([[p.x, p.y] for p in instance.points])\n",
    "\n",
    "#                     min_x = max(np.nanmin(points[:, 0]) - self.padding, 0)\n",
    "#                     min_y = max(np.nanmin(points[:, 1]) - self.padding, 0)\n",
    "#                     max_x = min(np.nanmax(points[:, 0]) + self.padding, w)\n",
    "#                     max_y = min(np.nanmax(points[:, 1]) + self.padding, h)\n",
    "\n",
    "#                     bbox = [min_x, min_y, max_x, max_y]\n",
    "\n",
    "#                 bboxes.append(bbox)\n",
    "\n",
    "#                 # crops\n",
    "#                 if self.crop_type == \"centroid\":\n",
    "#                     crop = crop_bbox(img, bbox)\n",
    "#                 elif self.crop_type == \"pose\":\n",
    "#                     crop = resize_and_pad(crop_bbox(img, bbox), self.crop_size)\n",
    "\n",
    "#                 crops.append(crop)\n",
    "\n",
    "#             instances.append(\n",
    "#                 {\n",
    "#                     \"video_id\": torch.from_numpy(np.array([label_idx])),\n",
    "#                     \"img\": torch.Tensor(img),\n",
    "#                     \"img_shape\": torch.from_numpy(np.array([img.shape])),\n",
    "#                     \"frame_id\": torch.from_numpy(np.array([i])),\n",
    "#                     \"num_detected\": torch.from_numpy(np.array([len(bboxes)])),\n",
    "#                     \"gt_track_ids\": torch.Tensor(gt_track_ids).type(torch.int64),\n",
    "#                     \"poses\": torch.Tensor(np.array(poses)),\n",
    "#                     \"bboxes\": torch.Tensor(np.array(bboxes)),\n",
    "#                     \"crops\": torch.stack(crops),\n",
    "#                     \"features\": torch.Tensor([]),\n",
    "#                     \"pred_track_ids\": torch.Tensor([-1 for _ in range(len(bboxes))]),\n",
    "#                     \"asso_output\": torch.Tensor([]),\n",
    "#                     \"matches\": torch.Tensor([]),\n",
    "#                     \"traj_score\": torch.Tensor([]),\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#         return instances\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     train_slp_files = [\"/Volumes/talmodata/datasets/mot/animal/sleap/benchmarks/flies13/190719_090330_wt_18159206_rig1.2@15000-17560.mp4\"]\n",
    "\n",
    "#     labels = sleap_io.load_slp(train_slp_files[0])\n",
    "\n",
    "#     all_anchors = labels.skeletons[0].node_names\n",
    "\n",
    "#     main_anchors = ['thorax', 'head', 'abdomen']\n",
    "#     anchors = main_anchors + [i for i in all_anchors if i not in main_anchors]\n",
    "\n",
    "#     device = 'cpu'\n",
    "\n",
    "#     feats = 256\n",
    "\n",
    "#     embedding_meta = {\n",
    "#             'embedding_type': 'fixed_pos',\n",
    "#             'kwargs': {\n",
    "#                 'temperature': 2,\n",
    "#                 'scale': 32,\n",
    "#                 'normalize': True\n",
    "#                 # 'learn_pos_num': 16,\n",
    "#                 # 'over_boxes': True\n",
    "#             }\n",
    "#     }\n",
    "\n",
    "#     tracking_transformer = GlobalTrackingTransformer(\n",
    "#         d_model=feats,\n",
    "#         num_encoder_layers=1,\n",
    "#         num_decoder_layers=1,\n",
    "#         dim_feedforward=feats,\n",
    "#         feature_dim_attn_head=feats,\n",
    "#         embedding_meta=embedding_meta,\n",
    "#         return_embedding=True\n",
    "#     ).to(device)\n",
    "\n",
    "#     asso_loss = AssoLoss().to(device)\n",
    "\n",
    "#     optimizer = torch.optim.Adam(\n",
    "#         tracking_transformer.parameters(), lr=1e-4, betas=(0.9, 0.999)\n",
    "#     )\n",
    "\n",
    "\n",
    "#     train_ds = AnimalDataset(\n",
    "#         train_slp_files,\n",
    "#         padding=5,\n",
    "#         crop_size=128,\n",
    "#         anchor_names=anchors,\n",
    "#         chunk=True,\n",
    "#         clip_length=32,\n",
    "#         crop_type=\"centroid\",\n",
    "#     )\n",
    "\n",
    "#     instances = next(iter(train_ds))\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         train_ds,\n",
    "#         batch_size=1,\n",
    "#         shuffle=True,\n",
    "#         collate_fn=train_ds.no_batching_fn,\n",
    "#         num_workers=0,\n",
    "#     )\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "#     num_epochs = 1\n",
    "\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         print(\"Epoch: {:02d}/{:02d}\".format(epoch, num_epochs))\n",
    "\n",
    "#         losses = []\n",
    "#         _ = tracking_transformer.train()\n",
    "\n",
    "#         print(\"TRAIN\")\n",
    "#         loop = tqdm(\n",
    "#             enumerate(train_loader), position=0, leave=True, total=len(train_loader)\n",
    "#         )\n",
    "#         for i, instances in loop:\n",
    "#             instances = instances[0]  # For batch size of 1.\n",
    "\n",
    "#             asso_preds, embedding = tracking_transformer(instances)\n",
    "\n",
    "#             # Compute loss.\n",
    "#             loss = asso_loss(asso_preds, instances)\n",
    "#             print('loss: ', loss)\n",
    "#             losses.append(loss.item())\n",
    "\n",
    "#             loop.set_description(\n",
    "#                 \"current_loss: {:.5f} | LR: {:.5f}\".format(\n",
    "#                     loss.item(), optimizer.param_groups[0][\"lr\"]\n",
    "#                 )\n",
    "#             )\n",
    "#             loop.set_postfix(loss=np.mean(losses))\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             break\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c1ca9-8dab-4465-9801-9069dbeece20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfa412-414a-4e0d-8025-cf440b21357f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biogtr",
   "language": "python",
   "name": "biogtr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
